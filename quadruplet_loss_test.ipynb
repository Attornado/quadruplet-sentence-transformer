{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import/install libraries"
   ],
   "metadata": {
    "id": "BP_7FrKqjtpw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIQSH7FUwRAy",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1687087543921,
     "user_tz": -120,
     "elapsed": 14701,
     "user": {
      "displayName": "Andrea Terlizzi (Attornado)",
      "userId": "13744383165512775606"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d6257fdb-6e68-4c0e-b415-01bfc2008c23"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting einops\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.2/42.2 kB\u001B[0m \u001B[31m1.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "import torch\n",
    "from torch.nn import TripletMarginWithDistanceLoss\n",
    "from torch.nn.functional import triplet_margin_loss, binary_cross_entropy_with_logits\n",
    "from typing import final, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We denote **anchor examples** with $\\mathbf{x}_a$, **positive examples** with $\\mathbf{x}_{pos}$, **negative examples** with $\\mathbf{x}_{neg}$ and **partially positive examples** with $\\mathbf{x}_{part}$.\n",
    "\n",
    "Let $\\gamma \\in [0, 1]$, $\\alpha_{pos\\text{-}neg}, \\alpha_{part\\text{-}neg}, \\alpha_{pos\\text{-}part}, \\lambda \\in \\mathbb{R}^+$, $E_{\\eta}$ be a text encoder; we define $\\gamma$*-quadruplet loss*:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}_{\\gamma} \\left(\\eta\\right) := \\mathbb{E}_{\\mathbf{x}_a, \\mathbf{x}_{pos}, \\mathbf{x}_{part} \\mathbf{x}_{neg}}\\big[ \\, & \\max\\left(||\\hat{\\mathbf{x}}_{a} - \\hat{\\mathbf{x}}_{pos}|| - ||\\hat{\\mathbf{x}}_{a} -\\hat{\\mathbf{x}}_{neg}|| + \\alpha_{pos\\text{-}neg}, 0\\right) + \\\\ & \\gamma \\max\\left(||\\hat{\\mathbf{x}}_{a} - \\hat{\\mathbf{x}}_{part}|| - ||\\hat{\\mathbf{x}}_{a} -\\hat{\\mathbf{x}}_{neg}|| + \\alpha_{part\\text{-}neg}, 0\\right) + \\\\ & (1 - \\gamma) \\max\\left(||\\hat{\\mathbf{x}}_{a} - \\hat{\\mathbf{x}}_{pos}|| - ||\\hat{\\mathbf{x}}_{a} -\\hat{\\mathbf{x}}_{part}|| + \\alpha_{pos\\text{-}part}, 0\\right)\\big]\n",
    "\\end{align*}\n",
    "where:\n",
    "\n",
    "\\begin{align*}\n",
    "    & \\hat{\\mathbf{x}}_{a} := E_{\\eta}\\left(\\mathbf{x}_{a}\\right) \\\\\n",
    "    & \\hat{\\mathbf{x}}_{pos} := E_{\\eta}\\left(\\mathbf{x}_{pos}\\right) \\\\\n",
    "    & \\hat{\\mathbf{x}}_{part} := E_{\\eta}\\left(\\mathbf{x}_{part}\\right) \\\\\n",
    "   & \\hat{\\mathbf{x}}_{neg} := E_{\\eta}\\left(\\mathbf{x}_{neg}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "The following function computes the $\\gamma$*-quadruplet loss*."
   ],
   "metadata": {
    "id": "oZU-CQVsjyBD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DEFAULT_GAMMA: final = 0.8\n",
    "REDUCTIONS: final = frozenset([\"mean\", \"sum\", \"none\"])\n",
    "\n",
    "\n",
    "def gamma_quadruplet_loss(x_anchor: torch.Tensor,\n",
    "                          x_pos: torch.Tensor,\n",
    "                          x_part: torch.Tensor,\n",
    "                          x_neg: torch.Tensor,\n",
    "                          gamma: float = DEFAULT_GAMMA,\n",
    "                          margin_pos_neg: float = 1.0,\n",
    "                          margin_pos_part: float = 1.0,\n",
    "                          margin_part_neg: float = 1.0,\n",
    "                          p: float = 2.0,\n",
    "                          swap: bool = False,\n",
    "                          reduction: str  = \"mean\") -> torch.Tensor:\n",
    "  if gamma < 0 or gamma > 1:\n",
    "    raise ValueError(f\"gamma must be between 0 and 1, {gamma} given\")\n",
    "  if margin_pos_neg <= 0:\n",
    "    raise ValueError(f\"margin_pos_neg must be positive, {margin_pos_neg} given\")\n",
    "  if margin_pos_part <= 0:\n",
    "    raise ValueError(f\"margin_pos_part must be positive, {margin_pos_part} given\")\n",
    "  if margin_part_neg <= 0:\n",
    "    raise ValueError(f\"margin_part_neg must be positive, {margin_part_neg} given\")\n",
    "  if reduction not in REDUCTIONS:\n",
    "    raise ValueError(f\"reduction must be one of: {REDUCTIONS}, \"\n",
    "                     f\"{reduction} given\")\n",
    "  if p <= 0:\n",
    "    raise ValueError(f\"p must be positive, {p} given\")\n",
    "\n",
    "  # Compute the triplet losses with no reduction, shape (B,)\n",
    "  a = triplet_margin_loss(\n",
    "      anchor=x_anchor,\n",
    "      positive=x_pos,\n",
    "      negative=x_neg,\n",
    "      margin=margin_pos_neg,\n",
    "      p=p,\n",
    "      swap=swap,\n",
    "      reduction='none'\n",
    "  )\n",
    "  b = triplet_margin_loss(\n",
    "      anchor=x_anchor,\n",
    "      positive=x_part,\n",
    "      negative=x_neg,\n",
    "      margin=margin_part_neg,\n",
    "      p=p,\n",
    "      swap=swap,\n",
    "      reduction='none'\n",
    "  )\n",
    "  c = triplet_margin_loss(\n",
    "      anchor=x_anchor,\n",
    "      positive=x_pos,\n",
    "      negative=x_part,\n",
    "      margin=margin_pos_part,\n",
    "      p=p,\n",
    "      swap=swap,\n",
    "      reduction='none'\n",
    "  )\n",
    "\n",
    "  # Return the reduced loss if required\n",
    "  if reduction == 'none':\n",
    "    return a + gamma*b + (1 -gamma)*c\n",
    "  elif reduction == 'sum':\n",
    "    return a.sum() + (gamma*b).sum() + ((1 -gamma)*c).sum()\n",
    "  else:\n",
    "    return a.mean() + (gamma*b).mean() + ((1 -gamma)*c).mean()"
   ],
   "metadata": {
    "id": "1HRSJ1VjjeQu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let $D_{\\theta}$ be a **binary discriminator** distinguishing positive examples from partially positive examples, parametrized by $\\mathbf{\\eta}$ and $\\mathbf{\\theta}$, respectively. We thus define $D_{\\theta}$*-regularized quadruplet loss*:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}_{D} \\left(\\eta, \\theta\\right):= \\mathbb{E}_{\\mathbf{x}_a, \\mathbf{x}_{pos}, \\mathbf{x}_{part} \\mathbf{x}_{neg}}\\big[ \\, & \\max\\left(||\\hat{\\mathbf{x}}_{a} - \\hat{\\mathbf{x}}_{pos}|| - ||\\hat{\\mathbf{x}}_{a} -\\hat{\\mathbf{x}}_{neg}|| + \\alpha_{pos\\text{-}neg}, 0 \\right) + \\\\ & \\max\\left(||\\hat{\\mathbf{x}}_{a} - \\hat{\\mathbf{x}}_{part}|| -||\\hat{\\mathbf{x}}_{a} - \\hat{\\mathbf{x}}_{neg}|| + \\alpha_{part\\textbf{-}neg}, 0\\right)+ \\\\ & - \\lambda \\log\\left(D_{\\theta}\\left(\\hat{\\mathbf{x}}_a, \\hat{\\mathbf{x}}_{pos}\\right)\\right) - \\lambda \\log\\left(1 - D_{\\theta}\\left(\\hat{\\mathbf{x}}_a, \\hat{\\mathbf{x}}_{part}\\right)\\right) \\big]\n",
    "\\end{align*}\n",
    "\n",
    "The following function computes the $D_θ$*-regularized quadruplet loss*."
   ],
   "metadata": {
    "id": "V8fOOyLqwVNf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def d_regularized_quadruplet_loss(\n",
    "                          x_anchor: torch.Tensor,\n",
    "                          x_pos: torch.Tensor,\n",
    "                          x_part: torch.Tensor,\n",
    "                          x_neg: torch.Tensor,\n",
    "                          margin_pos_neg: float = 1.0,\n",
    "                          margin_part_neg: float = 1.0,\n",
    "                          lmbd: float = 0.1,\n",
    "                          discr: Optional[torch.nn.Module] = None,\n",
    "                          discr_logits_pos: Optional[torch.Tensor] = None,\n",
    "                          discr_logits_part: Optional[torch.Tensor] = None,\n",
    "                          p: float = 2.0,\n",
    "                          swap: bool = False,\n",
    "                          reduction: str  = \"mean\") -> torch.Tensor:\n",
    "  if lmbd <= 0:\n",
    "    raise ValueError(f\"lmbd must be positive, {lmbd} given\")\n",
    "  if margin_pos_neg <= 0:\n",
    "    raise ValueError(f\"margin_pos_neg must be positive, {margin_pos_neg} given\")\n",
    "  if margin_part_neg <= 0:\n",
    "    raise ValueError(f\"margin_part_neg must be positive, {margin_part_neg} given\")\n",
    "  if reduction not in REDUCTIONS:\n",
    "    raise ValueError(f\"reduction must be one of: {REDUCTIONS}, \"\n",
    "                     f\"{reduction} given\")\n",
    "  if p <= 0:\n",
    "    raise ValueError(f\"p must be positive, {p} given\")\n",
    "  if discr is None and (discr_logits_part is None or discr_logits_pos is None):\n",
    "    raise ValueError(f\"Either discriminator or discriminator logits must be \"\n",
    "                      f\"given\")\n",
    "\n",
    "  # Compute the triplet losses with no reduction, shape (B,)\n",
    "  a = triplet_margin_loss(\n",
    "      anchor=x_anchor,\n",
    "      positive=x_pos,\n",
    "      negative=x_neg,\n",
    "      margin=margin_pos_neg,\n",
    "      p=p,\n",
    "      swap=swap,\n",
    "      reduction='none'\n",
    "  )\n",
    "  b = triplet_margin_loss(\n",
    "      anchor=x_anchor,\n",
    "      positive=x_part,\n",
    "      negative=x_neg,\n",
    "      margin=margin_part_neg,\n",
    "      p=p,\n",
    "      swap=swap,\n",
    "      reduction='none'\n",
    "  )\n",
    "\n",
    "  # Compute logits if required, with shape (B, 1)\n",
    "  if discr_logits_pos is None or discr_logits_part is None:\n",
    "    discr_logits_pos = discr(x_anchor, x_pos)\n",
    "    discr_logits_part = discr(x_anchor, x_part)\n",
    "\n",
    "  # Unsqueeze logits to obtain tensors with shape (B, 1, 1)\n",
    "  discr_logits_pos = discr_logits_pos.unsqueeze(1)\n",
    "  discr_logits_part = discr_logits_part.unsqueeze(1)\n",
    "\n",
    "  # Concatenate the logits and create targets with the same shape (B, 2, 1)\n",
    "  discr_logits_cat = torch.cat([discr_logits_pos, discr_logits_part], dim=1)\n",
    "  target_pos = torch.ones_like(discr_logits_pos)\n",
    "  target_part = torch.zeros_like(discr_logits_part)\n",
    "  target_cat = torch.cat([target_pos, target_part], dim=1)\n",
    "\n",
    "  # Calculate BCE loss with no reduction, shape (B, 2, 1)\n",
    "  bce = binary_cross_entropy_with_logits(discr_logits_cat, target=target_cat,\n",
    "                                         reduction='none')\n",
    "\n",
    "  # Sum loss value over the 2-th dim, obtaining tensor with shape (B, 1, 1)\n",
    "  bce = bce.sum(dim=1, keepdim=True)\n",
    "\n",
    "  print(bce.shape)\n",
    "\n",
    "  # Return the reduced loss if required\n",
    "  if reduction == 'none':\n",
    "    return a + b - lmbd*bce.squeeze(dim=-1)\n",
    "  elif reduction == 'sum':\n",
    "    return a.sum() + b.sum() - lmbd*bce.squeeze(dim=-1).sum()\n",
    "  else:\n",
    "    return a.mean() + b.mean() - lmbd*bce.squeeze(dim=-1).mean()"
   ],
   "metadata": {
    "id": "nwhRzL_0ylwg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following blocks just show the functioning of the above routines."
   ],
   "metadata": {
    "id": "sVrr-bUsliOG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pos = torch.ones(5)\n",
    "neg = torch.zeros(5)\n",
    "pos = pos.unsqueeze(-1)\n",
    "neg = neg.unsqueeze(-1)\n",
    "print(pos)\n",
    "print(neg)\n",
    "\n",
    "cat = torch.cat([pos, neg], dim=-1)\n",
    "print(cat.shape)\n",
    "print(cat)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zu6RPkJimY3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1687087543925,
     "user_tz": -120,
     "elapsed": 16,
     "user": {
      "displayName": "Andrea Terlizzi (Attornado)",
      "userId": "13744383165512775606"
     }
    },
    "outputId": "4e464af6-f3c0-40d9-b6ad-337d122d255a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "torch.Size([5, 2])\n",
      "tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class DummyDiscriminator(torch.nn.Module):\n",
    "  def __init__(self, in_channels: int):\n",
    "    super().__init__()\n",
    "    self._lin = torch.nn.Linear(in_channels*2, 1)\n",
    "\n",
    "  def forward(self, x_anchor: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    x = torch.cat([x_anchor, x], dim=-1)\n",
    "    del x_anchor\n",
    "    print(x.shape)\n",
    "    return self._lin(x)\n",
    "\n",
    "batch_size = 5\n",
    "l = DummyDiscriminator(10)\n",
    "x_anchor = torch.randn(batch_size, 10)\n",
    "x_pos = torch.randn(batch_size, 10)\n",
    "x_part = torch.randn(batch_size, 10)\n",
    "x_neg = torch.randn(batch_size, 10)\n",
    "print(x_pos.shape)\n",
    "print(x_part.shape)\n",
    "\n",
    "pred_logits_pos = l(x_anchor, x_pos).unsqueeze(1)\n",
    "pred_logits_part = l(x_part, x_pos).unsqueeze(1)\n",
    "print(pred_logits_pos.shape)\n",
    "print(pred_logits_part.shape)\n",
    "\n",
    "\n",
    "pred_logits_cat = torch.cat([pred_logits_pos, pred_logits_part], dim=1)\n",
    "target_pos = torch.ones_like(pred_logits_pos)\n",
    "target_part = torch.zeros_like(pred_logits_part)\n",
    "target_cat = torch.cat([target_pos, target_part], dim=1)\n",
    "print(pred_logits_cat)\n",
    "print(pred_logits_cat.shape)\n",
    "print(target_cat)\n",
    "print(target_cat.shape)\n",
    "\n",
    "bce_raw = binary_cross_entropy_with_logits(pred_logits_cat, target=target_cat, reduction='none')\n",
    "print(bce_raw)\n",
    "print(bce_raw.shape)\n",
    "bce_sum = bce_raw.sum(dim=1)\n",
    "print(bce_sum)\n",
    "print(bce_sum.shape)\n",
    "print(bce_sum.mean())\n",
    "bce = binary_cross_entropy_with_logits(pred_logits_cat, target=target_cat, reduction='mean')\n",
    "print(bce)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HsFwSya5kQEZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1687087543925,
     "user_tz": -120,
     "elapsed": 13,
     "user": {
      "displayName": "Andrea Terlizzi (Attornado)",
      "userId": "13744383165512775606"
     }
    },
    "outputId": "09212f0e-8675-4a8e-a323-216cfc28d0bd"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5, 10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5, 20])\n",
      "torch.Size([5, 20])\n",
      "torch.Size([5, 1, 1])\n",
      "torch.Size([5, 1, 1])\n",
      "tensor([[[-0.2870],\n",
      "         [ 0.2598]],\n",
      "\n",
      "        [[ 0.1180],\n",
      "         [-0.1036]],\n",
      "\n",
      "        [[ 0.4508],\n",
      "         [ 0.0024]],\n",
      "\n",
      "        [[-0.6488],\n",
      "         [-0.9238]],\n",
      "\n",
      "        [[-0.7359],\n",
      "         [-0.2133]]], grad_fn=<CatBackward0>)\n",
      "torch.Size([5, 2, 1])\n",
      "tensor([[[1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [0.]]])\n",
      "torch.Size([5, 2, 1])\n",
      "tensor([[[0.8469],\n",
      "         [0.8314]],\n",
      "\n",
      "        [[0.6359],\n",
      "         [0.6427]],\n",
      "\n",
      "        [[0.4930],\n",
      "         [0.6943]],\n",
      "\n",
      "        [[1.0693],\n",
      "         [0.3343]],\n",
      "\n",
      "        [[1.1273],\n",
      "         [0.5922]]], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "torch.Size([5, 2, 1])\n",
      "tensor([[1.6784],\n",
      "        [1.2786],\n",
      "        [1.1873],\n",
      "        [1.4036],\n",
      "        [1.7195]], grad_fn=<SumBackward1>)\n",
      "torch.Size([5, 1])\n",
      "tensor(1.4535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.7267, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "gql = gamma_quadruplet_loss(x_anchor, x_pos, x_part, x_neg, gamma=0.8, margin_pos_neg=1.0, margin_pos_part=0.9, margin_part_neg=0.8, p=2, reduction='none')\n",
    "print(gql)\n",
    "print(gql.shape)\n",
    "print(gql.mean())\n",
    "\n",
    "gql = gamma_quadruplet_loss(x_anchor, x_pos, x_part, x_neg, gamma=0.8, margin_pos_neg=1.0, margin_pos_part=0.9, margin_part_neg=0.8, p=2, reduction='sum')\n",
    "print(gql)\n",
    "print(gql/batch_size)\n",
    "print(gql.shape)\n",
    "\n",
    "gql = gamma_quadruplet_loss(x_anchor, x_pos, x_part, x_neg, gamma=0.8, margin_pos_neg=1.0, margin_pos_part=0.9, margin_part_neg=0.8, p=2, reduction='mean')\n",
    "print(gql)\n",
    "print(gql.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvUuHjT62-aY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1687087543925,
     "user_tz": -120,
     "elapsed": 11,
     "user": {
      "displayName": "Andrea Terlizzi (Attornado)",
      "userId": "13744383165512775606"
     }
    },
    "outputId": "e0572faf-fabb-481c-b03c-2af89a2a4b43"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([4.6508, 0.3866, 3.0095, 3.2603, 3.3247])\n",
      "torch.Size([5])\n",
      "tensor(2.9264)\n",
      "tensor(14.6319)\n",
      "tensor(2.9264)\n",
      "torch.Size([])\n",
      "tensor(2.9264)\n",
      "torch.Size([])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "gdl = d_regularized_quadruplet_loss(x_anchor, x_pos, x_part, x_neg, lmbd=0.8, margin_pos_neg=1.0, margin_part_neg=0.9, p=2, reduction='none', discr=l)\n",
    "print(gdl)\n",
    "print(gdl.shape)\n",
    "print(gdl.mean())\n",
    "\n",
    "gdl = d_regularized_quadruplet_loss(x_anchor, x_pos, x_part, x_neg, lmbd=0.8, margin_pos_neg=1.0, margin_part_neg=0.9, p=2, reduction='sum', discr=l)\n",
    "print(gdl)\n",
    "print(gdl/batch_size)\n",
    "print(gdl.shape)\n",
    "\n",
    "gdl = d_regularized_quadruplet_loss(x_anchor, x_pos, x_part, x_neg, lmbd=0.8, margin_pos_neg=1.0, margin_part_neg=0.9, p=2, reduction='mean', discr=l)\n",
    "print(gdl)\n",
    "print(gdl.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwmKDWAb6v-G",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1687087543926,
     "user_tz": -120,
     "elapsed": 9,
     "user": {
      "displayName": "Andrea Terlizzi (Attornado)",
      "userId": "13744383165512775606"
     }
    },
    "outputId": "29171f77-b04a-47dd-b6d8-79445e87c218"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5, 20])\n",
      "torch.Size([5, 20])\n",
      "torch.Size([5, 1, 1])\n",
      "tensor([[ 3.8286, -0.8132,  2.2484,  2.2924,  2.4306],\n",
      "        [ 3.6114, -1.0305,  2.0311,  2.0751,  2.2134],\n",
      "        [ 3.7003, -0.9415,  2.1201,  2.1641,  2.3023],\n",
      "        [ 3.4315, -1.2104,  1.8512,  1.8952,  2.0335],\n",
      "        [ 3.5944, -1.0475,  2.0141,  2.0581,  2.1963]], grad_fn=<SubBackward0>)\n",
      "torch.Size([5, 5])\n",
      "tensor(1.8020, grad_fn=<MeanBackward0>)\n",
      "torch.Size([5, 20])\n",
      "torch.Size([5, 20])\n",
      "torch.Size([5, 1, 1])\n",
      "tensor(9.0098, grad_fn=<SubBackward0>)\n",
      "tensor(1.8020, grad_fn=<DivBackward0>)\n",
      "torch.Size([])\n",
      "torch.Size([5, 20])\n",
      "torch.Size([5, 20])\n",
      "torch.Size([5, 1, 1])\n",
      "tensor(1.8020, grad_fn=<SubBackward0>)\n",
      "torch.Size([])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "discr_logits_pos = l(x_anchor, x_pos)\n",
    "discr_logits_part = l(x_anchor, x_part)\n",
    "gdl = d_regularized_quadruplet_loss(x_anchor, x_pos, x_part, x_neg, lmbd=0.8, margin_pos_neg=1.0, margin_part_neg=0.9, p=2, reduction='none', discr_logits_pos=discr_logits_pos, discr_logits_part=discr_logits_part)\n",
    "print(gdl)\n",
    "print(gdl.shape)\n",
    "print(gdl.mean())\n",
    "\n",
    "gdl = d_regularized_quadruplet_loss(x_anchor, x_pos, x_part, x_neg, lmbd=0.8, margin_pos_neg=1.0, margin_part_neg=0.9, p=2, reduction='sum', discr_logits_pos=discr_logits_pos, discr_logits_part=discr_logits_part)\n",
    "print(gdl)\n",
    "print(gdl/batch_size)\n",
    "print(gdl.shape)\n",
    "\n",
    "gdl = d_regularized_quadruplet_loss(x_anchor, x_pos, x_part, x_neg, lmbd=0.8, margin_pos_neg=1.0, margin_part_neg=0.9, p=2, reduction='mean', discr_logits_pos=discr_logits_pos, discr_logits_part=discr_logits_part)\n",
    "print(gdl)\n",
    "print(gdl.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yd8d31Qa9e9U",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1687087544417,
     "user_tz": -120,
     "elapsed": 5,
     "user": {
      "displayName": "Andrea Terlizzi (Attornado)",
      "userId": "13744383165512775606"
     }
    },
    "outputId": "fabff616-a0aa-421d-c6b3-906a278f7468"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5, 20])\n",
      "torch.Size([5, 20])\n",
      "torch.Size([5, 1, 1])\n",
      "tensor([[ 3.8286, -0.8132,  2.2484,  2.2924,  2.4306],\n",
      "        [ 3.6114, -1.0305,  2.0311,  2.0751,  2.2134],\n",
      "        [ 3.7003, -0.9415,  2.1201,  2.1641,  2.3023],\n",
      "        [ 3.4315, -1.2104,  1.8512,  1.8952,  2.0335],\n",
      "        [ 3.5944, -1.0475,  2.0141,  2.0581,  2.1963]], grad_fn=<SubBackward0>)\n",
      "torch.Size([5, 5])\n",
      "tensor(1.8020, grad_fn=<MeanBackward0>)\n",
      "torch.Size([5, 1, 1])\n",
      "tensor(9.0098, grad_fn=<SubBackward0>)\n",
      "tensor(1.8020, grad_fn=<DivBackward0>)\n",
      "torch.Size([])\n",
      "torch.Size([5, 1, 1])\n",
      "tensor(1.8020, grad_fn=<SubBackward0>)\n",
      "torch.Size([])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "t = torch.randn(5, 1, 2, 1)\n",
    "t = t.squeeze(dim=-1)\n",
    "t.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ReffTgY7ji1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1687087547430,
     "user_tz": -120,
     "elapsed": 761,
     "user": {
      "displayName": "Andrea Terlizzi (Attornado)",
      "userId": "13744383165512775606"
     }
    },
    "outputId": "6ed29e48-8a50-4973-eb23-0374997bdb64"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class QuadrupletLoss(torch.nn.Module, ABC):\n",
    "  def __init__(self,\n",
    "               margin_pos_neg: float = 1.0,\n",
    "               margin_pos_part: float = 1.0,\n",
    "               p: float = 2.0,\n",
    "               swap: bool = False,\n",
    "               reduction: str  = \"mean\"):\n",
    "    super().__init__()\n",
    "    if margin_pos_neg <= 0:\n",
    "      raise ValueError(f\"margin_pos_neg must be positive, {margin_pos_neg} given\")\n",
    "    if margin_pos_part <= 0:\n",
    "      raise ValueError(f\"margin_pos_part must be positive, {margin_pos_part} given\")\n",
    "    if reduction not in REDUCTIONS:\n",
    "      raise ValueError(f\"reduction must be one of: {REDUCTIONS}, \"\n",
    "                      f\"{reduction} given\")\n",
    "    if p <= 0:\n",
    "      raise ValueError(f\"p must be positive, {p} given\")\n",
    "\n",
    "\n",
    "    self.__margin_pos_neg: float = margin_pos_neg\n",
    "    self.__margin_pos_part: float = margin_pos_part\n",
    "    self.__p: float = p\n",
    "    self.__swap: bool = swap\n",
    "    self.__reduction: str = reduction\n",
    "\n",
    "  @property\n",
    "  def margin_pos_neg(self) -> float:\n",
    "    return self.__margin_pos_neg\n",
    "\n",
    "  @margin_pos_neg.setter\n",
    "  def margin_pos_neg(self, margin_pos_neg: float):\n",
    "    if margin_pos_neg <= 0:\n",
    "      raise ValueError(f\"margin_pos_neg must be positive, {margin_pos_neg} given\")\n",
    "    self.__margin_pos_neg = margin_pos_neg\n",
    "\n",
    "  @property\n",
    "  def margin_pos_part(self) -> float:\n",
    "    return self.__margin_pos_part\n",
    "\n",
    "  @margin_pos_part.setter\n",
    "  def margin_pos_part(self, margin_pos_part: float):\n",
    "    if margin_pos_part <= 0:\n",
    "      raise ValueError(f\"margin_pos_part must be positive, {margin_pos_part} given\")\n",
    "    self.__margin_pos_part = margin_pos_part\n",
    "\n",
    "  @property\n",
    "  def p(self) -> float:\n",
    "    return self.__p\n",
    "\n",
    "  @p.setter\n",
    "  def p(self, p: float):\n",
    "    if p <= 0:\n",
    "      raise ValueError(f\"p must be positive, {p} given\")\n",
    "    self.__p\n",
    "\n",
    "  @property\n",
    "  def swap(self) -> bool:\n",
    "    return self.__swap\n",
    "\n",
    "  @swap.setter\n",
    "  def swap(self, swap: bool):\n",
    "    self.__swap = swap\n",
    "\n",
    "  @property\n",
    "  def reduction(self) -> str:\n",
    "    return self.__reduction\n",
    "\n",
    "  @reduction.setter\n",
    "  def reduction(self, reduction: str):\n",
    "    if reduction not in REDUCTIONS:\n",
    "      raise ValueError(f\"reduction must be one of: {REDUCTIONS}, \"\n",
    "                      f\"{reduction} given\")\n",
    "    self.__reduction = reduction\n",
    "\n",
    "  @abstractmethod\n",
    "  def forward(self,\n",
    "              x_anchor: torch.Tensor,\n",
    "              x_pos: torch.Tensor,\n",
    "              x_part: torch.Tensor,\n",
    "              x_neg: torch.Tensor,\n",
    "              reduction: Optional[str] = None,\n",
    "              **kwargs) -> torch.Tensor:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "class GammaQuadrupletLoss(QuadrupletLoss):\n",
    "  def __init__(self,\n",
    "               gamma: float = DEFAULT_GAMMA,\n",
    "               margin_pos_neg: float = 1.0,\n",
    "               margin_pos_part: float = 1.0,\n",
    "               margin_part_neg: float = 1.0,\n",
    "               p: float = 2.0,\n",
    "               swap: bool = False,\n",
    "               reduction: str  = \"mean\"):\n",
    "    super().__init__(margin_pos_part=margin_pos_part,\n",
    "                     margin_pos_neg=margin_pos_neg,\n",
    "                     p=p,\n",
    "                     swap=swap,\n",
    "                     reduction=reduction)\n",
    "    if gamma < 0 or gamma > 1:\n",
    "      raise ValueError(f\"gamma must be between 0 and 1, {gamma} given\")\n",
    "    if margin_part_neg <= 0:\n",
    "      raise ValueError(f\"margin_part_neg must be positive, {margin_part_neg} given\")\n",
    "\n",
    "    self.__gamma: float = gamma\n",
    "    self.__margin_part_neg: float = margin_part_neg\n",
    "\n",
    "  @property\n",
    "  def gamma(self) -> float:\n",
    "    return self.__gamma\n",
    "\n",
    "  @gamma.setter\n",
    "  def gamma(self, gamma: float):\n",
    "    if gamma < 0 or gamma > 1:\n",
    "      raise ValueError(f\"gamma must be between 0 and 1, {gamma} given\")\n",
    "    self.__gamma = gamma\n",
    "\n",
    "  @property\n",
    "  def margin_part_neg(self) -> float:\n",
    "    return self.__margin_part_neg\n",
    "\n",
    "  @margin_part_neg.setter\n",
    "  def margin_part_neg(self, margin_part_neg: float):\n",
    "    if margin_part_neg <= 0:\n",
    "      raise ValueError(f\"margin_part_neg must be positive, {margin_part_neg} given\")\n",
    "    self.__margin_part_neg = margin_part_neg\n",
    "\n",
    "\n",
    "  def forward(self,\n",
    "              x_anchor: torch.Tensor,\n",
    "              x_pos: torch.Tensor,\n",
    "              x_part: torch.Tensor,\n",
    "              x_neg: torch.Tensor,\n",
    "              reduction: Optional[str] = None,\n",
    "              **kwargs) -> torch.Tensor:\n",
    "\n",
    "    reduction = self.reduction if reduction is None else reduction\n",
    "\n",
    "    return gamma_quadruplet_loss(x_anchor=x_anchor,\n",
    "                                 x_pos=x_pos,\n",
    "                                 x_part=x_part,\n",
    "                                 x_neg=x_neg,\n",
    "                                 gamma=self.gamma,\n",
    "                                 margin_pos_neg=self.margin_pos_neg,\n",
    "                                 margin_pos_part=self.margin_pos_part,\n",
    "                                 margin_part_neg=self.margin_part_neg,\n",
    "                                 p=self.p,\n",
    "                                 swap=self.swap,\n",
    "                                 reduction=reduction)\n",
    "\n",
    "gql_object = GammaQuadrupletLoss(gamma=0.8,\n",
    "                                 margin_pos_neg=1.0,\n",
    "                                 margin_pos_part=0.9,\n",
    "                                 margin_part_neg=0.8,\n",
    "                                 p=2,\n",
    "                                 reduction='none')\n",
    "\n",
    "gql = gql_object(x_anchor, x_pos, x_part, x_neg)\n",
    "print(gql)\n",
    "print(gql.shape)\n",
    "print(gql.mean())\n",
    "\n",
    "gql = gql_object(x_anchor, x_pos, x_part, x_neg, reduction='sum')\n",
    "print(gql)\n",
    "print(gql.shape)\n",
    "print(gql/batch_size)\n",
    "\n",
    "gql = gql_object(x_anchor, x_pos, x_part, x_neg, reduction='mean')\n",
    "print(gql.shape)\n",
    "print(gql)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F3A6J36BhIki",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1687087550319,
     "user_tz": -120,
     "elapsed": 406,
     "user": {
      "displayName": "Andrea Terlizzi (Attornado)",
      "userId": "13744383165512775606"
     }
    },
    "outputId": "7f64255e-2efa-49e5-ef50-1b847a8aba56"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([4.6508, 0.3866, 3.0095, 3.2603, 3.3247])\n",
      "torch.Size([5])\n",
      "tensor(2.9264)\n",
      "tensor(14.6319)\n",
      "torch.Size([])\n",
      "tensor(2.9264)\n",
      "torch.Size([])\n",
      "tensor(2.9264)\n"
     ]
    }
   ]
  }
 ]
}
